{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path='dataset'\n",
    "auto='auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory /Users/baptistebeaurain/Google Drive/learning/Python/polars_explo/project/airbnb/notebook\n",
      "\n",
      "Parent directory /Users/baptistebeaurain/Google Drive/learning/Python/polars_explo/project/airbnb\n",
      "\n",
      "Dataset path ['/Users/baptistebeaurain/Google Drive/learning/Python/polars_explo/project/airbnb/dataset/listings_austin.csv', '/Users/baptistebeaurain/Google Drive/learning/Python/polars_explo/project/airbnb/dataset/listings_bangkok.csv', '/Users/baptistebeaurain/Google Drive/learning/Python/polars_explo/project/airbnb/dataset/listings_buenos_aires.csv', '/Users/baptistebeaurain/Google Drive/learning/Python/polars_explo/project/airbnb/dataset/listings_cape_town.csv', '/Users/baptistebeaurain/Google Drive/learning/Python/polars_explo/project/airbnb/dataset/listings_istanbul.csv', '/Users/baptistebeaurain/Google Drive/learning/Python/polars_explo/project/airbnb/dataset/listings_melbourne.csv']\n",
      "\n",
      "Dump path /Users/baptistebeaurain/Google Drive/learning/Python/polars_explo/project/airbnb/auto\n"
     ]
    }
   ],
   "source": [
    "# get current directory\n",
    "path = os.getcwd()\n",
    "print(\"Current Directory\", path)\n",
    "print()\n",
    " \n",
    "# parent directory\n",
    "parent = os.path.dirname(path)\n",
    "print(\"Parent directory\", parent)\n",
    "print()\n",
    "\n",
    "# dataset path\n",
    "austin_path=os.path.join(parent,local_path,'listings_austin.csv')\n",
    "bangkok_path=os.path.join(parent,local_path,'listings_bangkok.csv')\n",
    "buenos_aires_path=os.path.join(parent,local_path,'listings_buenos_aires.csv')\n",
    "cape_town_path=os.path.join(parent,local_path,'listings_cape_town.csv')\n",
    "istanbul_path=os.path.join(parent,local_path,'listings_istanbul.csv')\n",
    "melbourne_path=os.path.join(parent,local_path,'listings_melbourne.csv')\n",
    "\n",
    "dataset_path=[austin_path,bangkok_path,buenos_aires_path,cape_town_path,istanbul_path,melbourne_path]\n",
    "print(\"Dataset path\",dataset_path)\n",
    "print()\n",
    "\n",
    "# auto path\n",
    "path_auto=os.path.join(parent,auto)\n",
    "print(\"Dump path\",path_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'df_austin': {'dataframe': 'df_austin', 'shape': (11269, 18)}, 'df_bangkok': {'dataframe': 'df_bangkok', 'shape': (17431, 18)}, 'df_buenos_aires': {'dataframe': 'df_buenos_aires', 'shape': (17671, 18)}, 'df_cape_town': {'dataframe': 'df_cape_town', 'shape': (16891, 18)}, 'df_istanbul': {'dataframe': 'df_istanbul', 'shape': (22539, 18)}, 'df_melbourne': {'dataframe': 'df_melbourne', 'shape': (18016, 18)}}\n"
     ]
    }
   ],
   "source": [
    "df_austin=pl.read_csv(austin_path)\n",
    "df_bangkok=pl.read_csv(bangkok_path)\n",
    "df_buenos_aires=pl.read_csv(buenos_aires_path)\n",
    "df_cape_town=pl.read_csv(cape_town_path)\n",
    "df_istanbul=pl.read_csv(istanbul_path)\n",
    "df_melbourne=pl.read_csv(melbourne_path)\n",
    "\n",
    "datasets=[df_austin,df_bangkok,df_buenos_aires,df_cape_town,df_istanbul,df_melbourne]\n",
    "\n",
    "\n",
    "## Function to return dataframes shapes\n",
    "def get_dataframes_and_shapes(dataframes):\n",
    "    result = {}\n",
    "    \n",
    "    for df in dataframes:\n",
    "        df_name = [name for name, frame in globals().items() if frame is df][0]\n",
    "        result[df_name] = {'dataframe': df_name, 'shape': df.shape}\n",
    "    \n",
    "    return result\n",
    "\n",
    "##################\n",
    "\n",
    "print(get_dataframes_and_shapes(datasets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure each datasets have the same columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['latitude', 'price', 'longitude', 'calculated_host_listings_count', 'name', 'number_of_reviews_ltm', 'number_of_reviews', 'host_id', 'host_name', 'neighbourhood_group', 'neighbourhood', 'minimum_nights', 'availability_365', 'license', 'id', 'last_review', 'room_type', 'reviews_per_month'], [])\n"
     ]
    }
   ],
   "source": [
    "## Function to compare dataframes columns\n",
    "def compare_columns(dataframes):\n",
    "    if not dataframes:\n",
    "        # If the list is empty, return empty lists\n",
    "        return [], []\n",
    "    \n",
    "    reference_columns = set(dataframes[0].columns)\n",
    "    \n",
    "    common_columns = set(dataframes[0].columns)\n",
    "    different_columns = set()\n",
    "    \n",
    "    for df in dataframes[1:]:\n",
    "        common_columns = common_columns.intersection(set(df.columns))\n",
    "        different_columns = different_columns.union(set(df.columns) - reference_columns)\n",
    "    \n",
    "    return list(common_columns), list(different_columns)\n",
    "\n",
    "##################\n",
    "\n",
    "print(compare_columns(datasets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure each column are the same type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function checking that all dataframes in a dataframe list are the same stype\n",
    "def check_consistent_column_types(dataframes):\n",
    "    if not dataframes:\n",
    "        return []\n",
    "\n",
    "    # Extract the column names from the first dataset\n",
    "    column_names = dataframes[0].columns\n",
    "\n",
    "    # Collect columns with inconsistent types\n",
    "    inconsistent_columns = []\n",
    "\n",
    "    for column_name in column_names:\n",
    "        first_column_type = dataframes[0][column_name].dtype\n",
    "\n",
    "        for dataset in dataframes[1:]:\n",
    "            if column_name in dataset.columns:\n",
    "                if dataset[column_name].dtype != first_column_type:\n",
    "                    inconsistent_columns.append(column_name)\n",
    "                    break  # No need to check further for this column\n",
    "\n",
    "    return inconsistent_columns\n",
    "\n",
    "##################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neighbourhood']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_consistent_column_types(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert columns of type not matching into String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_columns_to_string(dataframes, column_names):\n",
    "    \n",
    "    output=[]\n",
    "\n",
    "    for i, dataset in enumerate(dataframes):\n",
    "        for column_name in column_names:\n",
    "            # print(column_name)\n",
    "            if column_name in dataset.columns:\n",
    "                \n",
    "                col_name_old=column_name + \"_old\"  \n",
    "                # Create column as Str\n",
    "                dataset=dataset.with_columns(\n",
    "                    pl.col(column_name).cast(pl.Utf8, strict=False).alias(col_name_old)\n",
    "                )\n",
    "                # Remove old non Str column and rename the new\n",
    "                dataset=dataset.drop(column_name).rename({col_name_old:column_name})\n",
    "                output.append(dataset)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datasets_str=convert_columns_to_string(datasets, check_consistent_column_types(datasets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_datasets(dataset_list):\n",
    "    if not dataset_list:\n",
    "        return None\n",
    "    \n",
    "    # Assuming that all datasets have the same schema\n",
    "    concatenated_dataset = dataset_list[0]\n",
    "\n",
    "    for dataset in dataset_list[1:]:\n",
    "        concatenated_dataset = pl.concat([concatenated_dataset, dataset])\n",
    "\n",
    "    return concatenated_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103817, 18)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=concatenate_datasets(datasets_str)\n",
    "\n",
    "df.write_csv(os.path.join(path_auto,'concat.csv'))\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
